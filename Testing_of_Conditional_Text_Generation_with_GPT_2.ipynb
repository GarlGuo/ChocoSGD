{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlGuo/ChocoSGD/blob/master/Testing_of_Conditional_Text_Generation_with_GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwRhz144Fknp"
      },
      "source": [
        "Related article: https://www.ivanlai.project-ds.net/post/conditional-text-generation-by-fine-tuning-gpt-2\n",
        "\n",
        "Preprocessing code in [this](https://github.com/ivanlai/Conditional_Text_Generation) Github repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSWYe3LMSP-b"
      },
      "source": [
        "### Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xk9lb9GmZuS",
        "outputId": "e956d032-4236-4c87-c5fb-e081546f9e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 83.9 ms, sys: 25 ms, total: 109 ms\n",
            "Wall time: 9.97 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX5JEIimF82-"
      },
      "source": [
        "Check GPU memory available (Colab could offer 12GB or 16GB). \n",
        "\n",
        "Our configuration works on 16GB. The batch size needs to be reduced if only 12GB were available.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvdLF7jjREuv",
        "outputId": "b962b964-bdf7-4eaf-848e-1b85fd1614f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "8ca88b9e85024af8bd8e5fb52b2c5855",
            "abee785ba2f54e27918f9c0a0f1c7060",
            "e201e81d880b49b28d1cf0f15b7cb8a7",
            "e259541298e7498489cafd209c418734",
            "c4c8c74d09ce4ff9966389757581a793",
            "85d3a9455831483eab02fd30625a2a15",
            "07b8cae7a8bd433393baffb98896663c",
            "d5cb821fef0944b58032268e7c9094ef",
            "f17eead31a344208b76ed1175d96c08d",
            "e0143e2071464c51bca5a703443b0ce1",
            "41ad1776e5c3495284222669254b8dc8"
          ]
        },
        "id": "IL6-XP7zzH7h",
        "outputId": "c712ae95-e047-48a2-bf25-222409ff826f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ca88b9e85024af8bd8e5fb52b2c5855",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import zipfile\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import datetime\n",
        "from itertools import compress\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
        "                         AdamW, get_linear_schedule_with_warmup, \\\n",
        "                         TrainingArguments, BeamScorer, Trainer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
        "                             RandomSampler, SequentialSampler\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZDy9RClRiQ3"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QILzrXuoRhaF"
      },
      "outputs": [],
      "source": [
        "DEBUG           = False\n",
        "\n",
        "INPUT_DIR       = 'articles'\n",
        "\n",
        "USE_APEX        = True\n",
        "APEX_OPT_LEVEL  = 'O1'\n",
        "\n",
        "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
        "\n",
        "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
        "\n",
        "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
        "                    \"eos_token\": \"<|EOS|>\",\n",
        "                    \"unk_token\": \"<|UNK|>\",                    \n",
        "                    \"pad_token\": \"<|PAD|>\",\n",
        "                    \"sep_token\": \"<|SEP|>\",\n",
        "                    \"additional_special_tokens\":['[C1]','[C2]']}\n",
        "                    \n",
        "MAXLEN          = 768  #{768, 1024, 1280, 1600}\n",
        "\n",
        "TRAIN_SIZE      = 0.8\n",
        "\n",
        "if USE_APEX:\n",
        "    TRAIN_BATCHSIZE = 4\n",
        "    BATCH_UPDATE    = 16\n",
        "else:\n",
        "    TRAIN_BATCHSIZE = 2\n",
        "    BATCH_UPDATE    = 32\n",
        "\n",
        "EPOCHS          = 2\n",
        "LR              = 5e-4\n",
        "EPS             = 1e-8\n",
        "WARMUP_STEPS    = 1e2\n",
        "\n",
        "SEED            = 2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "740ZIyZXRbWe"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RpxktDOHpMI"
      },
      "source": [
        "### Download News Aggregator Dataset\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/News+Aggregator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDqrrihRHrVH"
      },
      "outputs": [],
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip'\n",
        "r = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tsFp19NmHv64",
        "outputId": "b1fa7411-44ab-45e7-8614-a4ad5171e69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df size: 422,419\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-833292a1-f4c5-471e-84c4-149f42c44107\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>URL</th>\n",
              "      <th>PUBLISHER</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>Alphanumeric ID</th>\n",
              "      <th>HOSTNAME Url</th>\n",
              "      <th>TIMESTAMP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Fed official says weak data caused by weather,...</td>\n",
              "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
              "      <td>Los Angeles Times</td>\n",
              "      <td>b</td>\n",
              "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
              "      <td>www.latimes.com</td>\n",
              "      <td>1394470370698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
              "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>b</td>\n",
              "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
              "      <td>www.livemint.com</td>\n",
              "      <td>1394470371207</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-833292a1-f4c5-471e-84c4-149f42c44107')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-833292a1-f4c5-471e-84c4-149f42c44107 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-833292a1-f4c5-471e-84c4-149f42c44107');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   ID                                              TITLE  \\\n",
              "0   1  Fed official says weak data caused by weather,...   \n",
              "1   2  Fed's Charles Plosser sees high bar for change...   \n",
              "\n",
              "                                                 URL          PUBLISHER  \\\n",
              "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
              "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
              "\n",
              "  CATEGORY                Alphanumeric ID      HOSTNAME Url      TIMESTAMP  \n",
              "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM   www.latimes.com  1394470370698  \n",
              "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.livemint.com  1394470371207  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns = ['ID',\n",
        "           'TITLE',\n",
        "           'URL',\n",
        "           'PUBLISHER',\n",
        "           'CATEGORY', #News category (b = business, t = science and technology, e = entertainment, m = health)\n",
        "           'Alphanumeric ID',\n",
        "           'HOSTNAME Url',\n",
        "           'TIMESTAMP']\n",
        "\n",
        "df = pd.read_csv(\"newsCorpora.csv\", sep='\\t', header=None, names=columns)\n",
        "print(f\"df size: {len(df) :,}\")\n",
        "\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHuvpmHFSrer"
      },
      "source": [
        "### Download articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhSFcfg4_kzY",
        "outputId": "a40aa1d3-c8f7-497b-9937-1becd25278f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FqqDZOVd8_LfOEA_2DEY5W70-DpQVF-F\n",
            "To: /content/news_articles.zip\n",
            "100% 49.2M/49.2M [00:00<00:00, 142MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Download news articles\n",
        "!gdown --id 1FqqDZOVd8_LfOEA_2DEY5W70-DpQVF-F\n",
        "!unzip -q '/content/news_articles.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bha20KNAGL11",
        "outputId": "ca828a46-695c-4ca1-9793-ce6cb4cc0fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(140) Items processed: 1,000/39,024; 0.3 minutes\n",
            "(140) Items processed: 2,000/39,024; 0.6 minutes\n",
            "(140) Items processed: 3,000/39,024; 0.8 minutes\n",
            "(140) Items processed: 4,000/39,024; 1.1 minutes\n",
            "(140) Items processed: 5,000/39,024; 1.4 minutes\n",
            "(140) Items processed: 6,000/39,024; 1.7 minutes\n",
            "(140) Items processed: 7,000/39,024; 1.9 minutes\n",
            "(140) Items processed: 8,000/39,024; 2.2 minutes\n",
            "(140) Items processed: 9,000/39,024; 2.5 minutes\n",
            "(140) Items processed: 10,000/39,024; 2.7 minutes\n",
            "(140) Items processed: 11,000/39,024; 3.0 minutes\n",
            "(140) Items processed: 12,000/39,024; 3.3 minutes\n",
            "(140) Items processed: 13,000/39,024; 3.6 minutes\n",
            "(140) Items processed: 14,000/39,024; 3.8 minutes\n",
            "(140) Items processed: 15,000/39,024; 4.1 minutes\n",
            "(140) Items processed: 16,000/39,024; 4.4 minutes\n",
            "(140) Items processed: 17,000/39,024; 4.7 minutes\n",
            "(140) Items processed: 18,000/39,024; 4.9 minutes\n",
            "(140) Items processed: 19,000/39,024; 5.2 minutes\n",
            "(140) Items processed: 20,000/39,024; 5.5 minutes\n",
            "(140) Items processed: 21,000/39,024; 5.7 minutes\n",
            "(140) Items processed: 22,000/39,024; 6.0 minutes\n",
            "(140) Items processed: 23,000/39,024; 6.3 minutes\n",
            "(140) Items processed: 24,000/39,024; 6.6 minutes\n",
            "(140) Items processed: 25,000/39,024; 6.8 minutes\n",
            "(140) Items processed: 26,000/39,024; 7.1 minutes\n",
            "(140) Items processed: 27,000/39,024; 7.3 minutes\n",
            "(140) Items processed: 28,000/39,024; 7.6 minutes\n",
            "(140) Items processed: 29,000/39,024; 7.9 minutes\n",
            "(140) Items processed: 30,000/39,024; 8.2 minutes\n",
            "(140) Items processed: 31,000/39,024; 8.5 minutes\n",
            "(140) Items processed: 32,000/39,024; 8.7 minutes\n",
            "(140) Items processed: 33,000/39,024; 9.0 minutes\n",
            "(140) Items processed: 34,000/39,024; 9.3 minutes\n",
            "(140) Items processed: 35,000/39,024; 9.6 minutes\n",
            "(140) Items processed: 36,000/39,024; 9.8 minutes\n",
            "(140) Items processed: 37,000/39,024; 10.1 minutes\n",
            "(140) Items processed: 38,000/39,024; 10.4 minutes\n",
            "(140) Items processed: 39,000/39,024; 10.6 minutes\n",
            "Number of articles: 39,024\n",
            "CPU times: user 10min 32s, sys: 7.9 s, total: 10min 40s\n",
            "Wall time: 10min 38s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "data = dict()            \n",
        "for root, dirs, files in os.walk(INPUT_DIR, topdown=True):\n",
        "    t0 = time.time()\n",
        "\n",
        "    for i, f in enumerate(files):\n",
        "        #id, category, title, keywords, text\n",
        "        id = int(f[:-4])        \n",
        "        tmp = df[['CATEGORY', 'TITLE']][df.ID==id].values\n",
        "        category, title = tmp[0][0], tmp[0][1]\n",
        "\n",
        "        with open(f'{INPUT_DIR}/{f}', \"r\") as infile:\n",
        "            text = infile.read()\n",
        "        \n",
        "        data[id] = [title, text]\n",
        "\n",
        "        if i%1000==0 and i>0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"({os.getpid()}) Items processed: {i :,}/{len(files):,}; {(time.time()-t0)/60 :.1f} minutes\")\n",
        "\n",
        "            if DEBUG:\n",
        "                break\n",
        "\n",
        "print(f\"Number of articles: {len(data) :,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1La_j8ZXoV2K"
      },
      "source": [
        "### Download Keywords \n",
        "Keywords of these articles have been extracted offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PTQq3OkzeaCG",
        "outputId": "dc39db9b-2542-43ea-c54d-5c637cc2b0b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C1WWvnt2egzhRmVXMSJhARz5GOLDGzMC\n",
            "To: /content/keywords.csv.zip\n",
            "100% 746k/746k [00:00<00:00, 37.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Download Keywords\n",
        "!gdown --id 1C1WWvnt2egzhRmVXMSJhARz5GOLDGzMC\n",
        "!unzip -q '/content/keywords.csv.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3MNo-mjNhEWx"
      },
      "outputs": [],
      "source": [
        "def read_keywords():\n",
        "    keywords = dict()\n",
        "    with open('keywords.csv', newline='') as f:\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        for row in reader:        \n",
        "            keywords[int(row[0])] = row[1:]          \n",
        "    print(f\"Number of entries in keywords: {len(keywords) :,}\")  \n",
        "    return keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pvxg8Q1_WtVl",
        "outputId": "6cbb24bc-dc31-4b86-9eb3-eb53d79ba453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of entries in keywords: 39,024\n",
            "Number of unique keywords: 29,291\n",
            "CPU times: user 164 ms, sys: 19.1 ms, total: 183 ms\n",
            "Wall time: 177 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "keywords = read_keywords()\n",
        "\n",
        "all_keywords = set()\n",
        "for k, v in keywords.items():\n",
        "    for w in v:\n",
        "        all_keywords.add(w)\n",
        "        \n",
        "for id in data.keys():\n",
        "    data[id].append(keywords[id])\n",
        "\n",
        "print(f\"Number of unique keywords: {len(all_keywords) :,}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j0V83HbH6QF"
      },
      "source": [
        "### Datasets and loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I8gp0I8JnMEE"
      },
      "outputs": [],
      "source": [
        "class myDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer, randomize=True):\n",
        "\n",
        "        title, text, keywords = [], [], []\n",
        "        for k, v in data.items():\n",
        "            title.append(v[0])\n",
        "            text.append(v[1])\n",
        "            keywords.append(v[2])\n",
        "\n",
        "        self.randomize = randomize\n",
        "        self.tokenizer = tokenizer \n",
        "        self.title     = title\n",
        "        self.text      = text\n",
        "        self.keywords  = keywords  \n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    @staticmethod\n",
        "    def join_keywords(keywords, randomize=True):\n",
        "        N = len(keywords)\n",
        "\n",
        "        #random sampling and shuffle\n",
        "        if randomize: \n",
        "            M = random.choice(range(N+1))\n",
        "            keywords = keywords[:M]\n",
        "            random.shuffle(keywords)\n",
        "\n",
        "        return ','.join(keywords)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        keywords = self.keywords[i].copy()\n",
        "        kw = self.join_keywords(keywords, self.randomize)\n",
        "        \n",
        "        input = SPECIAL_TOKENS['bos_token'] + SPECIAL_TOKENS['additional_special_tokens'][0] + SPECIAL_TOKENS['additional_special_tokens'][1] + \\\n",
        "                self.title[i] + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
        "                self.text[i] + SPECIAL_TOKENS['eos_token']\n",
        "\n",
        "        encodings_dict = tokenizer(input,                                   \n",
        "                                   truncation=True, \n",
        "                                   max_length=MAXLEN, \n",
        "                                   padding=\"max_length\")   \n",
        "        \n",
        "        input_ids = encodings_dict['input_ids']\n",
        "        attention_mask = encodings_dict['attention_mask']\n",
        "        \n",
        "        return {'label': torch.tensor(input_ids),\n",
        "                'input_ids': torch.tensor(input_ids), \n",
        "                'attention_mask': torch.tensor(attention_mask)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CwCXZgyrU7P5"
      },
      "outputs": [],
      "source": [
        "def split_data(data, S=TRAIN_SIZE):\n",
        "    # Shuffle ids\n",
        "    ids = list(data.keys())\n",
        "    random.shuffle(ids)\n",
        "\n",
        "    # Split into training and validation sets    \n",
        "    train_size = int(S * len(data))\n",
        "\n",
        "    train_ids = ids[:train_size]\n",
        "    val_ids = ids[train_size:]\n",
        "\n",
        "    train_data = dict()\n",
        "    for id in train_ids:\n",
        "        train_data[id] = data[id]\n",
        "\n",
        "    val_data = dict()\n",
        "    for id in val_ids:\n",
        "        val_data[id] = data[id]\n",
        "\n",
        "    return train_data, val_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3LfEbc5j9Yo"
      },
      "source": [
        "### Loading Tokenizer, Config and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "knL24TEIX9fl"
      },
      "outputs": [],
      "source": [
        "def get_tokenier(special_tokens=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
        "\n",
        "    if special_tokens:\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "        print(\"Special tokens added\")\n",
        "    return tokenizer\n",
        "\n",
        "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
        "\n",
        "    #GPT2LMHeadModel\n",
        "    if special_tokens:\n",
        "        config = AutoConfig.from_pretrained(MODEL, \n",
        "                                            bos_token_id=tokenizer.bos_token_id,\n",
        "                                            eos_token_id=tokenizer.eos_token_id,\n",
        "                                            sep_token_id=tokenizer.sep_token_id,\n",
        "                                            pad_token_id=tokenizer.pad_token_id,\n",
        "                                            output_hidden_states=False)\n",
        "    else: \n",
        "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
        "                                            pad_token_id=tokenizer.eos_token_id,\n",
        "                                            output_hidden_states=False)    \n",
        "\n",
        "    #----------------------------------------------------------------#\n",
        "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
        "\n",
        "    if special_tokens:\n",
        "        #Special tokens added, model needs to be resized accordingly\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if load_model_path:\n",
        "        model.load_state_dict(torch.load(load_model_path))\n",
        "\n",
        "    model.cuda()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "d74189e3a92b4294a3c5d89e0eba2abe",
            "4c03221d62484b49b68ff7a06bc30619",
            "6c72a92eb2e6411b9b878e640495de82",
            "78bfe5d8a1a94573ad4a8cb527d46354",
            "fded3a2c943f4f8c91192d214d8d52ae"
          ]
        },
        "id": "s2zrELuFTzEG",
        "outputId": "ae25fc40-f12e-41e7-b590-f80fef1933c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d74189e3a92b4294a3c5d89e0eba2abe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c03221d62484b49b68ff7a06bc30619",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c72a92eb2e6411b9b878e640495de82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78bfe5d8a1a94573ad4a8cb527d46354",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens added\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fded3a2c943f4f8c91192d214d8d52ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-2901be90f80c>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(tokenizer, special_tokens, load_model_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer, \n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                #   load_model_path='pytorch_model.bin'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8iGa-FRWAzWI",
        "outputId": "52b5bc80-5608-4761-ce18-45fc1306c5ae"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3e1149ec8a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# - Freeze selective layers:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# - Freeze all layers except last n:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# - Freeze selective layers:\n",
        "# - Freeze all layers except last n:\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "for i, m in enumerate(model.transformer.h):        \n",
        "    #Only un-freeze the last n transformer blocks\n",
        "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
        "        for parameter in m.parameters():\n",
        "            parameter.requires_grad = True \n",
        "\n",
        "for parameter in model.transformer.ln_f.parameters():        \n",
        "    parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.lm_head.parameters():        \n",
        "    parameter.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HHYTARXZOKwP"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = split_data(data)\n",
        "\n",
        "train_dataset = myDataset(train_data, tokenizer)\n",
        "val_dataset = myDataset(val_data, tokenizer, randomize=False)\n",
        "\n",
        "f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYh9gAM_lAxK"
      },
      "source": [
        "### Fine-tune GPT2 using Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GsKQJis8jcCh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
        "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
        "    gradient_accumulation_steps=BATCH_UPDATE,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    fp16_opt_level=APEX_OPT_LEVEL,\n",
        "    warmup_steps=WARMUP_STEPS,    \n",
        "    learning_rate=LR,\n",
        "    adam_epsilon=EPS,\n",
        "    weight_decay=0.01,        \n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=False,     \n",
        ")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,    \n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "trainer.train()\n",
        "trainer.save_model()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZYvCYf-zfs7V"
      },
      "outputs": [],
      "source": [
        "# Save to G-Drive ----------------------------------#\n",
        "# !cp -r 'pytorch_model.bin' '/content/drive/MyDrive/Colab Notebooks/Text Generation/pytorch_model_V2.bin'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0azvVPXCx4eM"
      },
      "source": [
        "### Generating text with Fine-tuned GPT-2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rM39_zQLhiZw"
      },
      "outputs": [],
      "source": [
        "# !cp -r '/content/drive/MyDrive/Colab Notebooks/Text Generation/pytorch_model_V2.bin' 'pytorch_model.bin' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dojGngEDRupX"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer, \n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                  load_model_path='pytorch_model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WYCC-ugJJy3A"
      },
      "outputs": [],
      "source": [
        "title = \"We got a lot of grief when our photo became a meme\"\n",
        "keywords = ['train', 'lads', 'drinking', 'picture', 'funny', 'instagram']\n",
        "kw = myDataset.join_keywords(keywords, randomize=False)\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token']\n",
        "        #   + title + \\\n",
        "        #  SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token']\n",
        "         \n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g1gM2DvGeh2i"
      },
      "outputs": [],
      "source": [
        "# Top-p (nucleus) text generation (10 samples):\n",
        "sample_outputs = model.generate(generated, \n",
        "                                do_sample=True,   \n",
        "                                min_length=50, \n",
        "                                max_length=MAXLEN,\n",
        "                                top_k=30,                                 \n",
        "                                top_p=0.7,        \n",
        "                                temperature=0.9,\n",
        "                                repetition_penalty=2.0,\n",
        "                                num_return_sequences=10\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "    a = len(title) + len(','.join(keywords))    \n",
        "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LEveXHOZnIGe"
      },
      "outputs": [],
      "source": [
        "# gen_sequences = sample_outputs.sequences[:, input_ids.shape[-1]:]\n",
        "generated.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BAmwMuxa3xGW"
      },
      "outputs": [],
      "source": [
        "# Beam-search text generation:\n",
        "sample_outputs = model.generate(generated, \n",
        "                                return_dict_in_generate=True,\n",
        "                                output_scores=True,\n",
        "                                # do_sample=True,   \n",
        "                                max_length=MAXLEN,                                                      \n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=5.0,\n",
        "                                early_stopping=True,      \n",
        "                                num_return_sequences=5\n",
        "                                )\n",
        "\n",
        "# for i, sample_output in enumerate(sample_outputs):\n",
        "#     text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "#     a = len(title) + len(','.join(keywords))    \n",
        "#     print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))\n",
        "\n",
        "# gen_sequences = sample_outputs.sequences[:, input_ids.shape[-1]:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6US2Lx5FzXU-"
      },
      "outputs": [],
      "source": [
        "gen_sequences = sample_outputs.sequences[:, generated.shape[-1]:]\n",
        "gen_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k7SA-wGw2xXY"
      },
      "outputs": [],
      "source": [
        "for i, gen_seq in enumerate(gen_sequences):\n",
        "  print(f\"{i}th gen_seq is:\")\n",
        "  print(tokenizer.decode(gen_seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VvIoBGSrzXM_"
      },
      "outputs": [],
      "source": [
        "probs = torch.stack(sample_outputs.scores, dim=1).softmax(-1)\n",
        "# print(sample_outputs.scores)\n",
        "probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ASYlig0R003-"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(SPECIAL_TOKENS[\"additional_special_tokens\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HrYSzz7z000K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lnoHgiT300xb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XnIEgYE500ul"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4UjMTwWx-ky"
      },
      "source": [
        "### Generating text with raw GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2ErmuqkQzVue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kbiaQldb1RPO"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenier()\n",
        "model = get_model(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H1ag9Z0iZbzG"
      },
      "outputs": [],
      "source": [
        "prompt = title\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval()\n",
        "sample_outputs = model.generate(generated, \n",
        "                                do_sample=True,   \n",
        "                                max_length=MAXLEN,                                                      \n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=5.0,\n",
        "                                early_stopping=True,      \n",
        "                                num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07b8cae7a8bd433393baffb98896663c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ad1776e5c3495284222669254b8dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d3a9455831483eab02fd30625a2a15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca88b9e85024af8bd8e5fb52b2c5855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abee785ba2f54e27918f9c0a0f1c7060",
              "IPY_MODEL_e201e81d880b49b28d1cf0f15b7cb8a7",
              "IPY_MODEL_e259541298e7498489cafd209c418734"
            ],
            "layout": "IPY_MODEL_c4c8c74d09ce4ff9966389757581a793"
          }
        },
        "abee785ba2f54e27918f9c0a0f1c7060": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d3a9455831483eab02fd30625a2a15",
            "placeholder": "",
            "style": "IPY_MODEL_07b8cae7a8bd433393baffb98896663c",
            "value": ""
          }
        },
        "c4c8c74d09ce4ff9966389757581a793": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5cb821fef0944b58032268e7c9094ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e0143e2071464c51bca5a703443b0ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e201e81d880b49b28d1cf0f15b7cb8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5cb821fef0944b58032268e7c9094ef",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f17eead31a344208b76ed1175d96c08d",
            "value": 0
          }
        },
        "e259541298e7498489cafd209c418734": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0143e2071464c51bca5a703443b0ce1",
            "placeholder": "",
            "style": "IPY_MODEL_41ad1776e5c3495284222669254b8dc8",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "f17eead31a344208b76ed1175d96c08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}